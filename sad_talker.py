# -*- coding: utf-8 -*-
"""sad_talker.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rOXdPUJRNCd4_FppzHRHQiWHNSbRhGNd
"""

!nvidia-smi --query-gpu=name,memory.total,memory.free --format=csv,noheader

# Commented out IPython magic to ensure Python compatibility.
!update-alternatives --install /usr/local/bin/python3 python3 /usr/bin/python3.8 2
!update-alternatives --install /usr/local/bin/python3 python3 /usr/bin/python3.9 1
!sudo apt install python3.8

!sudo apt-get install python3.8-distutils

!python --version

!apt-get update

!apt install software-properties-common

!sudo dpkg --remove --force-remove-reinstreq python3-pip python3-setuptools python3-wheel

!apt-get install python3-pip

print('Git clone project and install requirements...')
!git clone https://github.com/Winfredy/SadTalker &> /dev/null
# %cd SadTalker
!export PYTHONPATH=/content/SadTalker:$PYTHONPATH
!python3.8 -m pip install torch==1.12.1+cu113 torchvision==0.13.1+cu113 torchaudio==0.12.1 --extra-index-url https://download.pytorch.org/whl/cu113
!apt update
!apt install ffmpeg &> /dev/null
!python3.8 -m pip install -r requirements.txt

print('Download pre-trained models...')
!rm -rf checkpoints
!bash scripts/download_models.sh

import os
import google.generativeai as genai

# Set the API key directly in the notebook
os.environ["API_KEY"] = "AIzaSyAfoIujpOefg5QXMwEP9un8wFhnWehVlvI"

# Now configure the API
api_key = os.environ["API_KEY"]
genai.configure(api_key=api_key)

model = genai.GenerativeModel("gemini-1.5-flash")
response = model.generate_content("what is Artificial intelligence in three line")
print(response.text)

# Step 2: Upload your service account key file (JSON) to Colab
from google.colab import files
files.upload()  # Upload the JSON key file you downloaded from Google Cloud Console

# Step 3: Set the environment variable to point to the service account key file
import os
os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = "/content/drive/MyDrive/lunar-nuance-430514-k4-0aeddb30b136.json"  # replace with the name of your uploaded JSON file

# Step 4: Use the Google Cloud Text-to-Speech API
from google.cloud import texttospeech

client = texttospeech.TextToSpeechClient()

# Prepare the text for speech synthesis
input_text = texttospeech.SynthesisInput(text=response.text)

# Configure the voice parameters
voice = texttospeech.VoiceSelectionParams(
    language_code="en-US",
    name="en-US-Studio-O"
)

# Configure the audio parameters
audio_config = texttospeech.AudioConfig(
    audio_encoding=texttospeech.AudioEncoding.MP3,
    speaking_rate=1.05
)

# Synthesize speech
tts_response = client.synthesize_speech(
    request={"input": input_text, "voice": voice, "audio_config": audio_config}
)

# Save the audio file
file_path = "/content/output.mp3"
with open(file_path, "wb") as out:
    out.write(tts_response.audio_content)
    print('Audio content written to file "output.mp3"')

# borrow from makeittalk
import ipywidgets as widgets
import glob
import matplotlib.pyplot as plt
print("Choose the image name to animate: (saved in folder 'examples/')")
img_list = glob.glob1('/content/SadTalker/examples/source_image', '*.png')
img_list.sort()
img_list = [item.split('.')[0] for item in img_list]
default_head_name = widgets.Dropdown(options=img_list, value='art_6')
def on_change(change):
    if change['type'] == 'change' and change['name'] == 'value':
        plt.imshow(plt.imread('/content/SadTalker/examples/source_image/{}.png'.format(default_head_name.value)))
        plt.axis('off')
        plt.show()
default_head_name.observe(on_change)
display(default_head_name)
plt.imshow(plt.imread('/content/SadTalker/examples/source_image/{}.png'.format(default_head_name.value)))
plt.axis('off')
plt.show()

# selected audio from exmaple/driven_audio
img = '/content/SadTalker/examples/source_image/{}.png'.format(default_head_name.value)
print(img)
!python3.8 inference.py --driven_audio /content/output.mp3 \
           --source_image {img} \
           --result_dir ./results --still --preprocess full --enhancer gfpgan

from google.colab import drive
drive.mount('/content/drive')

# visualize code from makeittalk
from IPython.display import HTML
from base64 import b64encode
import os, sys

# get the last from results

results = sorted(os.listdir('./results/'))

mp4_name = glob.glob('./results/*.mp4')[0]

mp4 = open('{}'.format(mp4_name),'rb').read()
data_url = "data:video/mp4;base64," + b64encode(mp4).decode()

print('Display animation: {}'.format(mp4_name), file=sys.stderr)
display(HTML("""
  <video width=256 controls>
        <source src="%s" type="video/mp4">
  </video>
  """ % data_url))





img = '/content/SadTalker/examples/source_image/{}.png'.format(default_head_name.value)
print(img)
!python3.8 inference.py --driven_audio /content/output.mp3 --ref_pose ./examples/ref_video/WDA_KatieHill_000.mp4 --ref_eyeblink ./examples/ref_video/WDA_KatieHill_000.mp4 \
                        --source_image {img} \
                        --result_dir ./results --still --preprocess full --enhancer gfpgan --expression_scale 1.0

# visualize code from makeittalk
from IPython.display import HTML
from base64 import b64encode
import os, sys

# get the last from results

results = sorted(os.listdir('./results/'))

mp4_name = glob.glob('./results/*.mp4')[0]

mp4 = open('{}'.format(mp4_name),'rb').read()
data_url = "data:video/mp4;base64," + b64encode(mp4).decode()

print('Display animation: {}'.format(mp4_name), file=sys.stderr)
display(HTML("""
  <video width=256 controls>
        <source src="%s" type="video/mp4">
  </video>
  """ % data_url))